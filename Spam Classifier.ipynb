{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af96909b",
   "metadata": {},
   "source": [
    "# Spam Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2fe7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023b2438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = pd.read_csv(r'C:\\Users\\Admin\\Downloads\\SMSSpamCollection', sep = '\\t', names = ['label', 'message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279da78",
   "metadata": {},
   "source": [
    "Data Cleaning and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c2a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24011449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36859d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d2d3f",
   "metadata": {},
   "source": [
    "Creating a new variable corpus which is an empty list that will store all the sentences after text cleaning is done. \n",
    "\n",
    "1. Removing all unncessary characters (. , numbers ..).\n",
    "2. We use for loop that will run through all the messages. \n",
    "3. We will remove all characters except a-z and A-Z and replace it with ' ' blank.\n",
    "4. We are then going to lowercase all the words.\n",
    "5. We are then going to split all the sentences. Stemming will help us get the base root of every word\n",
    "6. From the above step we will get a list of base roots in review. \n",
    "7. We will then use the condition : for the word in review if that word is not present in stopwords.words under english then we will apply stemming process to get the base root of that word. \n",
    "8. So now we have a list of words in review, we are going to join these words using .join() into a sentence. \n",
    "9. We will then append this to corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f5529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6916011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043db7e8",
   "metadata": {},
   "source": [
    "Creating Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "400ebbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8afe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2954c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d95505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = pd.get_dummies(messages['label'])\n",
    "y = y.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5890fe4",
   "metadata": {},
   "source": [
    "Training, Testing and Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18892cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72818a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f280a6",
   "metadata": {},
   "source": [
    "Training the Model using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6838f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d4cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "728fb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = spam_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a4b320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979372197309417"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_detect_model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
